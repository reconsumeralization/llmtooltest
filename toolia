import os
import random
import json
import torch
from dataclasses import dataclass
from transformers import AutoTokenizer, BartConfig, BartForConditionalGeneration, AdamW
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import spacy

# Install and load the necessary packages
os.system("pip install spacy")
os.system("python -m spacy download en_core_web_sm")

nlp_models = ["en_core_web_sm"]
assist_algorithms = ["autopep8", "black", "isort", "rope", "jedi", "pylint", "mypy", "flake8", "pydocstyle", "bandit", "starcoder", "llmtool"]

def create_source_generator(model_name, algorithm):
    nlp = spacy.load(model_name)
    def generate_source(text, num_sentences):
        doc = nlp(text)
        sentences = [sent.text for sent in doc.sents]
        random.shuffle(sentences)
        generated_text = ". ".join(sentences[:num_sentences]) + "."
        return generated_text + "\n\n# Code assist by " + algorithm
    return generate_source

source_generators = []
for model_name in nlp_models:
    for algorithm in assist_algorithms:
        generator = create_source_generator(model_name, algorithm)
        source_generators.append(generator)

# Generate and save the training data
training_data = []
for generator in source_generators:
    generated_source = generator("This is a sample text.", 3)
    training_data.append({"input": "This is a sample text.", "output": generated_source})

# Save the training data to a JSON file
with open("training_data.json", "w") as f:
    json.dump(training_data, f)

# Define the LLMData class for storing training data
@dataclass
class LLMData:
    input_ids: torch.Tensor
    attention_mask: torch.Tensor
    labels: torch.Tensor

# Define the LLMModel class for training and generating LLM tools
class LLMModel:
    def __init__(self, model_name, device):
        self.model_name = model_name
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = BartForConditionalGeneration.from_pretrained(model_name).to(device)

    def preprocess_data(self, data):
        # Preprocess the data and convert it into LLMData format
        input_ids = []
        attention_mask = []
        labels = []
        for item in data:
            encoded_inputs = self.tokenizer.encode_plus(
                item["input"],
                padding="max_length",
                max_length=512,
                truncation=True,
                return_tensors="pt"
            )
            input_ids.append(encoded_inputs["input_ids"])
            attention_mask.append(encoded_inputs["attention_mask"])
            labels.append(self.tokenizer.encode(item["output"], add_special_tokens=False))
        
        input_ids = torch.cat(input_ids, dim=0).to(self.device)
        attention_mask = torch.cat(attention_mask, dim=0).to(self.device)
        labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(l) for l in labels], batch_first=True).to(self.device)
        
        return LLMData(input_ids, attention_mask, labels)

    def train(self, train_data, batch_size, num_epochs):
        # Train the LLM model
        train_dataset = self.preprocess_data(train_data)
        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

        optimizer = AdamW(self.model.parameters(), lr=1e-5)

        self.model.train()
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            for batch in tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{num_epochs}", unit="batch"):
                optimizer.zero_grad()
                outputs = self.model(
                    input_ids=batch.input_ids,
                    attention_mask=batch.attention_mask,
                    labels=batch.labels
                )
                loss = outputs.loss
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()

            print(f"Epoch {epoch+1} Loss: {epoch_loss / len(train_dataloader):.4f}")

    def generate_tool(self, prompt):
        # Generate a tool using the trained LLM model
        self.model.eval()
        input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
        output_ids = self.model.generate(input_ids, max_length=100, num_beams=4, early_stopping=True)
        tool = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        return tool

# Load training data
with open("training_data.json", "r") as f:
    train_data = json.load(f)

# Initialize LLM model
model = LLMModel(model_name="facebook/bart-base", device=torch.device("cpu"))

# Train the LLM model
print("Training the LLM model...")
model.train(train_data=train_data, batch_size=4, num_epochs=5)
print("LLM model training completed.")

# Generate a tool using the trained LLM model
prompt = "How to schedule a meeting?"
print("Generating a tool using the LLM model...")
tool = model.generate_tool(prompt)
print("Tool generated:", tool)
